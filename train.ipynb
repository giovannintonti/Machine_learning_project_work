{"cells":[{"cell_type":"markdown","metadata":{"id":"jg5AYXdmV6YI"},"source":["# Caricare le varie librerie pytorch e il modello scelto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KL-tL9RYVwRL"},"outputs":[],"source":["# Download del dataset dal drive\n","import gdown\n","def download_google_file(shader_url, output_name):\n","  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n","  gdown.download(id_url, output_name)\n","\n","# video del drive (in mp4)\n","download_google_file(\"https://drive.google.com/file/d/1tCMqO-PpKseWG_20bjYJwq80oziIY7t2/view?usp=drive_link\", \"VIDEOS.zip\")\n","!unzip VIDEOS.zip\n","\n","# gt del drive (in rtf)\n","download_google_file(\"https://drive.google.com/file/d/1BF1dlcI0DYdeciJzWtV0-wMOp5O_pks7/view?usp=drive_link\", \"GT.zip\")\n","!unzip GT.zip"]},{"cell_type":"markdown","metadata":{"id":"l9Xi26TOWCNa"},"source":["# Funzioni di estrazione dei frame dal video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwSrdj6ivMHz"},"outputs":[],"source":["# Path\n","videos_path = \"TRAINING_SET\"\n","frames_path = \"FRAMES\"\n","\n","#Pulizia folder\n","!rm -R FRAMES/TRAINING_SET/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSfjgF7SvE23"},"outputs":[],"source":["import cv2, os, argparse, glob, PIL, tqdm\n","from PIL import Image\n","def extract_frames(video):\n","    # Process the video\n","    ret = True\n","    cap = cv2.VideoCapture(video)\n","    f = 0\n","    while ret:\n","        ret, img = cap.read()\n","        if ret:\n","            f += 1\n","            Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f)))\n","    cap.release()\n","\n","# For all the videos\n","file_list = [path for path in glob.glob(os.path.join(videos_path,\"**\"), recursive=True)\n","             if os.path.isfile(path)]\n","print(file_list)\n","for video in tqdm.tqdm(file_list):\n","  if os.path.isdir(os.path.join(frames_path, video)):\n","    continue\n","\n","  os.makedirs(os.path.join(frames_path, video))\n","  extract_frames(video)\n","  #os.system(\"ffmpeg -i {} -r 10 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxOLICtI6iY3"},"outputs":[],"source":["# per leggere i file rtf\n","!pip install striprtf\n","!pip install pytorchvideo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WD0VmuLtw2Dm"},"outputs":[],"source":["import os\n","import os.path\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms\n","import torch\n","from typing import List, Union, Tuple, Any\n","from striprtf.striprtf import rtf_to_text\n","import albumentations\n","\n","\n","class VideoRecord(object):\n","    \"\"\"\n","    Helper class for class VideoFrameDataset. This class\n","    represents a video sample's metadata.\n","\n","    Args:\n","        root_datapath: the system path to the root folder of the videos.\n","        row: A list with four or more elements where\n","             1) The first element is the path to the video sample's frames excluding\n","             the root_datapath prefix\n","             2) The  second element is the starting frame id of the video\n","             3) The third element is the inclusive ending frame id of the video\n","             4) The fourth element is the label index.\n","             5) any following elements are labels in the case of multi-label classification\n","    \"\"\"\n","    def __init__(self, row, root_datapath):\n","        self._data = row\n","        self._path = os.path.join(root_datapath, row[0])\n","\n","    @property\n","    def path(self) -> str:\n","        return self._path\n","\n","    @property\n","    def num_frames(self) -> int:\n","        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n","\n","    @property\n","    def start_frame(self) -> int:\n","        return int(self._data[1])\n","\n","    @property\n","    def end_frame(self) -> int:\n","        return int(self._data[2])\n","\n","    @property\n","    def label(self) -> Union[int, List[int]]:\n","        # just one label_id\n","        if len(self._data) == 4:\n","            return int(self._data[3])\n","        # sample associated with multiple labels\n","        else:\n","            return [int(label_id) for label_id in self._data[3:]]\n","\n","\n","class VideoFrameDataset(torch.utils.data.Dataset):\n","    r\"\"\"\n","    A highly efficient and adaptable dataset class for videos.\n","    Instead of loading every frame of a video,\n","    loads x RGB frames of a video (sparse temporal sampling) and evenly\n","    chooses those frames from start to end of the video, returning\n","    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n","    tensors.\n","\n","    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n","    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n","\n","    Note:\n","        A demonstration of using this class can be seen\n","        in ``demo.py``\n","        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n","\n","    Note:\n","        This dataset broadly corresponds to the frame sampling technique\n","        introduced in ``Temporal Segment Networks`` at ECCV2016\n","        https://arxiv.org/abs/1608.00859.\n","\n","    Args:\n","        root_path: The root path in which video folders lie.\n","                   this is ROOT_DATA from the description above.\n","        num_segments: The number of segments the video should\n","                      be divided into to sample frames from.\n","        frames_per_segment: The number of frames that should\n","                            be loaded per segment. For each segment's\n","                            frame-range, a random start index or the\n","                            center is chosen, from which frames_per_segment\n","                            consecutive frames are loaded.\n","        imagefile_template: The image filename template that video frame files\n","                            have inside of their video folders as described above.\n","        transform: Transform pipeline that receives a list of numpy images/frames.\n","        test_mode: If True, frames are taken from the center of each\n","                   segment, instead of a random location in each segment.\n","\n","    \"\"\"\n","    def __init__(self,\n","                 root_path: str,\n","                 num_segments: int = 3,\n","                 frames_per_segment: int = 1,\n","                 imagefile_template: str='{:05d}.jpg',\n","                 transform=None,\n","                 totensor=True,\n","                 test_mode: bool = False):\n","        super(VideoFrameDataset, self).__init__()\n","\n","        self.root_path = root_path\n","        self.num_segments = num_segments\n","        self.frames_per_segment = frames_per_segment\n","        self.imagefile_template = imagefile_template\n","        self.test_mode = test_mode\n","\n","        if transform is None:\n","            self.transform = None\n","        else:\n","            additional_targets = {}\n","            for i in range(self.num_segments * self.frames_per_segment - 1):\n","                additional_targets[\"image%d\" % i] = \"image\"\n","            self.transform = albumentations.Compose([transform],\n","                                                    additional_targets=additional_targets,\n","                                                    p=1)\n","        self.totensor = totensor\n","        self.totensor_transform = ImglistOrdictToTensor()\n","\n","        self._parse_annotationfile()\n","        self._sanity_check_samples()\n","\n","    def _load_image(self, directory: str, idx: int) -> Image.Image:\n","        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n","\n","    def _parse_annotationfile(self):\n","        self.video_list = []\n","        for class_name in os.listdir(self.root_path):\n","            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n","                frames_dir = os.path.join(self.root_path, class_name, video_name)\n","                if os.path.isdir(frames_dir):\n","                    frame_path = os.path.join(class_name, video_name)\n","                    end_frame = len(os.listdir(frames_dir))\n","                    # E' stato aggiunto al codice questo if per effettuare la traduzione della\n","                    # stringa in modo da renderla compatibile con il codice già presente\n","                    # per ricavare il file rtf.\n","                    if(\"KFOLD\" in frames_dir):\n","                      split_string = frames_dir.split(\"/\")\n","                      split_string[1]=\"TRAINING_SET\"\n","                      frames_dir = \"/\".join(split_string[:2] + split_string[2:]).replace(\"KFOLD/\", \"FRAMES/\")\n","                    annotation_path = frames_dir\\\n","                        .replace(\"\\\\\", \"/\") \\\n","                        .replace(\"FRAMES/\", \"GT/\") \\\n","                        .replace(\".mp4\", \".rtf\")\n","\n","                    with open(annotation_path, 'r') as file:\n","                        text = rtf_to_text(file.read())\n","                    if len(text):\n","                        label = 1\n","                        start_frame = int(text.split(\",\")[0])\n","                        if start_frame == 0:\n","                          start_frame = 1\n","                    else:\n","                        label = 0\n","                        start_frame = 1\n","\n","                    self.video_list.append(VideoRecord(\n","                        [frame_path, start_frame, end_frame, label],\n","                        self.root_path))\n","\n","    def _sanity_check_samples(self):\n","        for record in self.video_list:\n","            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n","                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n","\n","            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n","                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n","                      f\"but the dataloader is set up to load \"\n","                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n","                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n","                      f\"error when trying to load this video.\\n\")\n","\n","    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n","        \"\"\"\n","        For each segment, choose a start index from where frames\n","        are to be loaded from.\n","\n","        Args:\n","            record: VideoRecord denoting a video sample.\n","        Returns:\n","            List of indices of where the frames of each\n","            segment are to be loaded from.\n","        \"\"\"\n","        # choose start indices that are perfectly evenly spread across the video frames.\n","        if self.test_mode:\n","            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n","\n","            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n","                                      for x in range(self.num_segments)])\n","        # randomly sample start indices that are approximately evenly spread across the video frames.\n","        else:\n","            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n","\n","            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n","                      np.random.randint(max_valid_start_index, size=self.num_segments)\n","\n","        return start_indices\n","\n","    def __getitem__(self, idx: int) -> Union[\n","        Tuple[List[Image.Image], Union[int, List[int]]],\n","        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n","        Tuple[Any, Union[int, List[int]]],\n","        ]:\n","        \"\"\"\n","        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n","        frames from evenly chosen locations across the video.\n","\n","        Args:\n","            idx: Video sample index.\n","        Returns:\n","            A tuple of (video, label). Label is either a single\n","            integer or a list of integers in the case of multiple labels.\n","            Video is either 1) a list of PIL images if no transform is used\n","            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n","            if the transform \"ImglistToTensor\" is used\n","            3) or anything else if a custom transform is used.\n","        \"\"\"\n","        record: VideoRecord = self.video_list[idx]\n","\n","        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n","\n","        return self._get(record, frame_start_indices)\n","\n","    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n","        Tuple[List[Image.Image], Union[int, List[int]]],\n","        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n","        Tuple[Any, Union[int, List[int]]],\n","        ]:\n","        \"\"\"\n","        Loads the frames of a video at the corresponding\n","        indices.\n","\n","        Args:\n","            record: VideoRecord denoting a video sample.\n","            frame_start_indices: Indices from which to load consecutive frames from.\n","        Returns:\n","            A tuple of (video, label). Label is either a single\n","            integer or a list of integers in the case of multiple labels.\n","            Video is either 1) a list of PIL images if no transform is used\n","            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n","            if the transform \"ImglistToTensor\" is used\n","            3) or anything else if a custom transform is used.\n","        \"\"\"\n","\n","        frame_start_indices = frame_start_indices + record.start_frame\n","        images = list()\n","\n","        # from each start_index, load self.frames_per_segment\n","        # consecutive frames\n","        for start_index in frame_start_indices:\n","            frame_index = int(start_index)\n","\n","            # load self.frames_per_segment consecutive frames\n","            for _ in range(self.frames_per_segment):\n","                image = self._load_image(record.path, frame_index)\n","                images.append(image)\n","\n","                if frame_index < record.end_frame:\n","                    frame_index += 1\n","\n","        if self.transform is not None:\n","            transform_input = {\"image\": images[0]}\n","            for i, image in enumerate(images[1:]):\n","                transform_input[\"image%d\" % i] = image\n","            images = self.transform(**transform_input)\n","\n","        if self.totensor:\n","            images = self.totensor_transform(images)\n","\n","        #È stato aggiunto il seguente frammento di codice per gestire l'input della rete SlowFaST_R50.\n","        #Questa rete prende in input una lista di due tensori: il primo rappresenta il percorso di Slow\n","        # e il secondo rappresenta il percorso di Fast. Viene restituita la lista e la label corrispondente\n","        frames_tensor = {'video': images.permute(1,0,2,3)}\n","        video_data = transform(frames_tensor)\n","        inputs = video_data[\"video\"]\n","        inputs = [i[None, ...].squeeze() for i in inputs]\n","        return inputs, record.label\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","\n","class ImglistOrdictToTensor(torch.nn.Module):\n","    \"\"\"\n","    Converts a list or a dict of numpy images to a torch.FloatTensor\n","    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n","    Can be used as first transform for ``VideoFrameDataset``.\n","    \"\"\"\n","    @staticmethod\n","    def forward(img_list_or_dict):\n","        \"\"\"\n","        Converts each numpy image in a list or a dict to\n","        a torch Tensor and stacks them into a single tensor.\n","\n","        Args:\n","            img_list_or_dict: list or dict of numpy images.\n","        Returns:\n","            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n","        \"\"\"\n","        if isinstance(img_list_or_dict, list):\n","            return torch.stack([transforms.functional.to_tensor(img)\n","                                for img in img_list_or_dict])\n","        else:\n","            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n","                                for k in img_list_or_dict.keys()])"]},{"cell_type":"markdown","metadata":{"id":"uZD2VxDSWe5V"},"source":["# Struttura del modello\n"]},{"cell_type":"markdown","source":["Questi sono i valori delle trasformazioni standard utilizzate per addestrare la SlowFast_R50. È importante notare che alcune di queste trasformazioni standard sono state rimosse per motivi specifici:\n","\n","1. Lambda: Questa operazione è gestita dalla classe ImglistOrdictToTensor.\n","2. ShortSideScale e CenterCrop: Sono state inserite nel preprocessing per migliorare le prestazioni.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"_64ki14Uwa_m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xq1HAL8bnH2t"},"outputs":[],"source":["#@title  { display-mode: \"form\" }\n","#@markdown Parametri SlowFast Transform\n","\n","\n","alpha = 4 #@param {type: \"number\"}\n","num_frames = 32 #@param {type: \"number\"}\n","sampling_rate = 2 #@param {type: \"number\"}\n","\n","#@markdown ---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSSMVNq0kRKX"},"outputs":[],"source":["####################\n","# SlowFast transform\n","####################\n","import torch\n","import json\n","from torchvision.transforms import Compose, Lambda\n","from torchvision.transforms._transforms_video import (\n","    CenterCropVideo,\n","    NormalizeVideo,\n",")\n","from pytorchvideo.data.encoded_video import EncodedVideo\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n","    UniformCropVideo\n",")\n","\n","mean = [0.45, 0.45, 0.45]\n","std = [0.225, 0.225, 0.225]\n","\n","class PackPathway(torch.nn.Module):\n","    \"\"\"\n","    Transform for converting video frames as a list of tensors.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, frames: torch.Tensor):\n","        fast_pathway = frames\n","        # Perform temporal sampling from the fast pathway.\n","        slow_pathway = torch.index_select(\n","            frames,\n","            1,\n","            torch.linspace(\n","                0, frames.shape[1] - 1, frames.shape[1] // alpha\n","            ).long(),\n","        )\n","        frame_list = [slow_pathway, fast_pathway]\n","        return frame_list\n","\n","transform =  ApplyTransformToKey(\n","    key=\"video\",\n","    transform=Compose(\n","        [\n","            UniformTemporalSubsample(num_frames),\n","            NormalizeVideo(mean, std),\n","            PackPathway()\n","        ]\n","    ),\n",")\n"]},{"cell_type":"markdown","source":["In questa funzione, carichiamo il modello. La prima volta che viene eseguita, il modello viene scaricato dalla rete. Dalle volte successive, il modello sarà già presente in memoria. Successivamente, randomizziamo i pesi dell'ultimo blocco e ripetiamo la stessa operazione per i sei layer precedenti.<br>\n","Abbiamo scelto di ricaricare il modello ogni volta per garantire maggiore sicurezza e ridurre il rischio di riutilizzare i pesi dal fold precedente."],"metadata":{"id":"2DxlTCW5uRLc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJx23NjDXJKF"},"outputs":[],"source":["def clear_model():\n","    \"\"\"\n","    Cancella gli ultimi layer del modello e ne aggiunge dei nuovi, impedendo il gradiente\n","    discendente sui layer precedenti (quelli già allenati dalla rete).\n","\n","    Args:\n","        model (Model): la rete da modificare.\n","\n","    Returns:\n","        Model: la rete modificata.\n","    \"\"\"\n","    # Carico la rete\n","    model_name = \"slowfast_r50\"\n","    model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n","    # Disattivo il gradiente\n","    model.blocks[:-1].requires_grad_(False)\n","    # Ultimo layer\n","    model.blocks[-1].proj = torch.nn.Sequential(torch.nn.Linear(2304,128),torch.nn.ReLU(),torch.nn.Linear(128,2))\n","    torch.nn.init.xavier_uniform_(model.blocks[-1].proj[0].weight)\n","    torch.nn.init.xavier_uniform_(model.blocks[-1].proj[2].weight)\n","    model.blocks[-1].proj.requires_grad_(True)\n","    model.blocks[-1].output_pool=  torch.nn.Softmax()\n","    # Septultimo layer\n","    model.blocks[-3].multipathway_blocks[1].res_blocks[1].branch2.conv_a.requires_grad_(True)\n","    torch.nn.init.xavier_uniform_(model.blocks[-3].multipathway_blocks[1].res_blocks[1].branch2.conv_a.weight)\n","    # Sestultimo layer\n","    model.blocks[-3].multipathway_blocks[1].res_blocks[1].branch2.conv_b.requires_grad_(True)\n","    torch.nn.init.xavier_uniform_(model.blocks[-3].multipathway_blocks[1].res_blocks[1].branch2.conv_b.weight)\n","    # Quintultimo layer\n","    model.blocks[-3].multipathway_blocks[1].res_blocks[1].branch2.conv_c.requires_grad_(True)\n","    torch.nn.init.xavier_uniform_(model.blocks[-3].multipathway_blocks[1].res_blocks[1].branch2.conv_c.weight)\n","    # Quartultimo layer\n","    model.blocks[-3].multipathway_blocks[1].res_blocks[2].branch2.conv_a.requires_grad_(True)\n","    torch.nn.init.xavier_uniform_(model.blocks[-3].multipathway_blocks[1].res_blocks[2].branch2.conv_a.weight)\n","    # Terzultimo layer\n","    model.blocks[-3].multipathway_blocks[1].res_blocks[2].branch2.conv_b.requires_grad_(True)\n","    torch.nn.init.xavier_uniform_(model.blocks[-3].multipathway_blocks[1].res_blocks[2].branch2.conv_b.weight)\n","    # Penultimo layer\n","    model.blocks[-3].multipathway_blocks[1].res_blocks[2].branch2.conv_c.requires_grad_(True)\n","    torch.nn.init.xavier_uniform_(model.blocks[-3].multipathway_blocks[1].res_blocks[2].branch2.conv_c.weight)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFt-hKmfWfQ3"},"outputs":[],"source":["import torch\n","import json\n","from torchvision.transforms import Compose, Lambda\n","from torchvision.transforms._transforms_video import (\n","    CenterCropVideo,\n","    NormalizeVideo,\n",")\n","from pytorchvideo.data.encoded_video import EncodedVideo\n","from typing import Dict\n","\n","#creazione modello\n","model=clear_model()"]},{"cell_type":"markdown","metadata":{"id":"jNQyMa7aYrrc"},"source":["# Gestione di KFold e di generazione dei dataloader\n"]},{"cell_type":"markdown","metadata":{"id":"8nN5wcgbK1cS"},"source":["Si è deciso di usare la move al posto della copytree per ragioni di efficienza del disco, la move fa semplicemente una ridenominazione nel filesystem, la copytree invece deve riscrivere ogni singolo file sul disco, impiegando quantità di tempo molto maggiori"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLkUc8GcyC4e"},"outputs":[],"source":["# Path\n","videos_path = \"TRAINING_SET\"\n","frames_path = \"FRAMES\"\n","initial_path=\"FRAMES/TRAINING_SET\"\n","kfold_path=\"KFOLD\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlv9yac2yYm1"},"outputs":[],"source":["import os\n","import random\n","import shutil\n","\n","def k_fold_split(folder_path,destination_path, k):\n","    \"\"\"\n","    Esegue la tecnica K-fold su una cartella contenente due sottocartelle etichettate come \"1\" e \"0\",\n","    prelevando in modo proporzionale da entrambe le cartelle.\n","\n","    Args:\n","        folder_path (str): Percorso completo della cartella dei fold.\n","        destination_path (str): percorso completo della cartella di destinazione.\n","        k (int): Numero di fold desiderati.\n","\n","    Returns:\n","        bool: True se la divisione K-fold è stata eseguita con successo, False altrimenti.\n","    \"\"\"\n","    if not os.path.isdir(folder_path):\n","        return False\n","\n","    label_folders = [\"0\", \"1\"]\n","    fold_names = [f\"fold_{i+1}\" for i in range(k)]\n","\n","    # Ottieni la lista dei file nella cartella etichettata \"1\"\n","    label_1_folder = os.path.join(folder_path, \"1\")\n","    label_1_files = os.listdir(label_1_folder)\n","\n","    # Ottieni la lista dei file nella cartella etichettata \"0\"\n","    label_0_folder = os.path.join(folder_path, \"0\")\n","    label_0_files = os.listdir(label_0_folder)\n","\n","    # Calcola il numero di file per ogni fold in modo proporzionale\n","    total_files = len(label_1_files) + len(label_0_files)\n","    files_per_fold = total_files // k\n","    label_1_files_per_fold = round(len(label_1_files) * files_per_fold / total_files)\n","    label_0_files_per_fold = round(len(label_0_files) * files_per_fold / total_files)\n","\n","    # Distribuisci casualmente i file etichettati \"1\" nei fold\n","    random.shuffle(label_1_files)\n","    for i, fold_name in enumerate(fold_names):\n","        destination_folder = os.path.join(destination_path, fold_name, \"1\")\n","        files_to_move = label_1_files[i * label_1_files_per_fold:(i+1) * label_1_files_per_fold]\n","        for file_name in files_to_move:\n","            source_file_path = os.path.join(label_1_folder, file_name)\n","            destination_file_path = os.path.join(destination_folder, file_name)\n","            shutil.move(source_file_path, destination_file_path)\n","\n","    # Recupero dei file rimanenti etichettati \"1\"\n","    label_1_files = os.listdir(label_1_folder)\n","    random.shuffle(label_1_files)\n","    dim=len(label_1_files)\n","    for i, fold_name in enumerate(fold_names):\n","      if dim!=0:\n","        destination_folder = os.path.join(destination_path, fold_name, \"1\")\n","        file_name = label_1_files.pop()\n","        source_file_path = os.path.join(label_1_folder, file_name)\n","        destination_file_path = os.path.join(destination_folder, file_name)\n","        shutil.move(source_file_path, destination_file_path)\n","        dim-=1\n","      else:\n","        break\n","\n","\n","    # Distribuisci casualmente i file etichettati \"0\" nei fold\n","    random.shuffle(label_0_files)\n","    for i, fold_name in enumerate(fold_names):\n","        destination_folder = os.path.join(destination_path, fold_name, \"0\")\n","        files_to_move = label_0_files[i * label_0_files_per_fold:(i+1) * label_0_files_per_fold]\n","        for file_name in files_to_move:\n","            source_file_path = os.path.join(label_0_folder, file_name)\n","            destination_file_path = os.path.join(destination_folder, file_name)\n","            shutil.move(source_file_path, destination_file_path)\n","\n","    # Recupero dei file rimanenti etichettati \"0\"\n","    label_0_files = os.listdir(label_0_folder)\n","    random.shuffle(label_0_files)\n","    dim=len(label_0_files)\n","    for i, fold_name in enumerate(fold_names):\n","      if dim!=0:\n","        destination_folder = os.path.join(destination_path, fold_name, \"0\")\n","        file_name = label_0_files.pop()\n","        source_file_path = os.path.join(label_0_folder, file_name)\n","        destination_file_path = os.path.join(destination_folder, file_name)\n","        shutil.move(source_file_path, destination_file_path)\n","        dim-=1\n","      else:\n","        break\n","\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9L3yZSIJzLW"},"outputs":[],"source":["import os\n","import shutil\n","\n","def restore_k_fold(folder_path,destination_path, k):\n","    \"\"\"\n","    Riporta tutti i file delle cartelle dei fold nella cartella originale.\n","\n","    Args:\n","        folder_path (str): Percorso completo della cartella dei fold.\n","        destination_path (str): percorso completo della cartella di destinazione.\n","        k (int): Numero di fold.\n","\n","    Returns:\n","        bool: True se i file sono stati riportati con successo, False altrimenti.\n","    \"\"\"\n","    if not os.path.isdir(folder_path):\n","        return False\n","\n","    fold_names = [f\"fold_{i+1}\" for i in range(k)]\n","\n","    # Riporta i file dei fold nella cartella originale\n","    for fold_name in fold_names:\n","        fold_path = os.path.join(folder_path, fold_name)\n","        for label_folder in os.listdir(fold_path):\n","            label_folder_path = os.path.join(fold_path, label_folder)\n","            for file_name in os.listdir(label_folder_path):\n","                source_file_path = os.path.join(label_folder_path, file_name)\n","                destination_file_path = os.path.join(destination_path, label_folder, file_name)\n","                shutil.move(source_file_path, destination_file_path)\n","\n","        # Elimina la cartella del fold\n","        shutil.rmtree(fold_path)\n","\n","    return True"]},{"cell_type":"markdown","metadata":{"id":"7V6n7IgmZcqC"},"source":["Si è deciso di gestire in questo modo il k-fold, creando diversi dataset per il training e non uno solo poichè preoccuparsi di fare quest'altro passaggio sarebbe costato in termini di tempo per la copia dei vari fold. Il risultato adottando questa situazione non cambia poichè crea un dataloader con tutti i dataset.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UclXYmWKXt1p"},"outputs":[],"source":["def make_training_validation_dataloaders(dir,k, validation_num,preprocessing,augumentation,num_segments,frames_per_segment,transform_probability,batch_size,num_workers):\n","    \"\"\"\n","      Restituisce una lista contenente i dataloader del training set e del\n","      validation set\n","\n","      Args:\n","          dir (str): Percorso completo della cartella dei fold.\n","          k (int): Numero di fold.\n","          validation_num (int): Numero del fold di validation.\n","\n","      Returns:\n","          [Dataloader, Datalaoader]: training e validation dataloaders.\n","      \"\"\"\n","    from tqdm import tqdm\n","    from torch.utils.data import DataLoader,ConcatDataset\n","\n","    if(validation_num>k):\n","      raise ValueError(\"k deve essere maggiore di validation_num!\")\n","\n","    # Dataset per il training\n","    training_sets=list()\n","    for i in range(1,k+1):\n","      if i!=validation_num:\n","        training_sets.append(VideoFrameDataset(root_path=dir+\"/fold_\"+str(i),\n","                                num_segments=num_segments,\n","                                frames_per_segment=frames_per_segment,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augumentation],\n","                                    p=transform_probability,\n","                                )\n","                                )\n","                                )\n","      else:\n","        validation_set=VideoFrameDataset(root_path=dir+\"/fold_\"+str(i),\n","                                num_segments=num_segments,\n","                                frames_per_segment=frames_per_segment,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augumentation],\n","                                    p=transform_probability,\n","                                ),\n","                                totensor=True,\n","                                test_mode=True,\n","                                )\n","    training_set = ConcatDataset(training_sets)\n","    dataloader_train = DataLoader(training_set, shuffle=True,\n","                              batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n","    dataloader_validation = DataLoader(validation_set, shuffle=False,\n","                              batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n","\n","    return dataloader_train, dataloader_validation"]},{"cell_type":"markdown","metadata":{"id":"ysUM2gG-bQxA"},"source":["# Definizione singola epoca"]},{"cell_type":"markdown","source":["Funzione realizzata per eseguire una singola epoca di training e per valutare le performance del validation.<br>\n","Vengono utilizzati diversi indici di prestazione:\n","1. Accuracy\n","2. Loss\n","3. Precision\n","4. Recall\n","5. F-score"],"metadata":{"id":"G-jGojdCx-Pm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"478i7E-6bRIY"},"outputs":[],"source":["from torch import Tensor\n","from torch.utils.data import DataLoader\n","from torch.optim import Optimizer\n","from torch.nn import Module\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","\n","def one_epoch( model, dataloader_train, dataloader_validation,\n","              lossFunction, optimizer, writer,\n","               epoch_num):\n","  ''' Esegue una epoca sul dataset di training ed effettua anche una validazione\n","    con il validation set\n","\n","\n","    Args:\n","\n","    model (Module): Il modello di rete neurale.\n","    dataloader_train (DataLoader): Il DataLoader per il dataset di addestramento.\n","    dataloader_validation (DataLoader): Il DataLoader per il dataset di validazione.\n","    lossFunction (Module): La funzione di loss da utilizzare durante l'addestramento.\n","    optimizer (Optimizer): L'ottimizzatore da utilizzare durante l'addestramento.\n","    writer (SummaryWriter): Oggetto SummaryWriter per scrivere i log su TensorBoard.\n","    epoch_num (int): Il numero di epoca corrente.\n","\n","\n","    Returns:\n","    accuracy_final (float): La media delle accuracy sul dataset di validazione.\n","    loss_final (float): La media delle loss sul dataset di validazione.\n","    precision_final (float): La media del valore di precision sul dataset di validazione.\n","    recall_final (float): La media del valore di recall sul dataset di validazione.\n","    f_score_final (float): La media del valore di F-score sul dataset di validazione.\n","    '''\n","\n","  # contatore per TensorBoard\n","  i_start = epoch_num * len(dataloader_train)\n","\n","  # --- TRAINING ---\n","  for i, (X, y) in enumerate(dataloader_train):\n","    X[0]=X[0].cuda()\n","    X[1]=X[1].cuda()\n","    y = y.cuda()\n","\n","    # azzero il gradiente\n","    optimizer.zero_grad()\n","\n","    # effettuo il forward e utilizzo la funzione di loss, effettuando il backpropagation\n","    output = model.forward(X)\n","    loss = lossFunction(output, y)\n","    loss.backward()\n","\n","    # utilizzo l'optimizer\n","    optimizer.step()\n","\n","    # Definisco il valore di positività\n","    positive = output[:, 1] >= 0.5\n","\n","    # calcolo gli indici di prestazione ed aggiorno il rispettivo contatore\n","    #true positive\n","    true_positive=(positive.detach()==True)&(y.detach()==1)\n","    true_positive_counter=true_positive.detach().float().count_nonzero().item()* 1.0\n","    #false positive\n","    false_positive=(positive.detach()==True)&(y.detach()==0)\n","    false_positive_counter=false_positive.detach().float().count_nonzero().item()* 1.0\n","    #true negative\n","    true_negative=(positive.detach()==False)&(y.detach()==0)\n","    true_negative_counter=true_negative.detach().float().count_nonzero().item()* 1.0\n","    #false negative\n","    false_negative=(positive.detach()==False)&(y.detach()==1)\n","    false_negative_counter=false_negative.detach().float().count_nonzero().item()* 1.0\n","\n","    # Calcolo la precision\n","    if true_positive_counter+false_positive_counter>0:\n","      precision=true_positive_counter/(true_positive_counter+false_positive_counter)\n","    else:\n","      precision=0\n","    # Calcolo la recall\n","    if true_positive_counter+false_negative_counter>0:\n","      recall=true_positive_counter/(true_positive_counter+false_negative_counter)\n","    else:\n","      recall=0\n","    # Calcolo lo F-score\n","    if precision+recall>0:\n","      f_score=2*precision*recall/(precision+recall)\n","    else:\n","      f_score=0\n","\n","    # Scrittura dei risutlati su TensorBoard\n","    # Train/Loss\n","    writer.add_scalar('train/loss', loss.detach().item(), i_start + i)\n","    # Train/Precision\n","    writer.add_scalar('train/precision', precision, i_start + i)\n","    # Train/Recall\n","    writer.add_scalar('train/recall', recall, i_start + i)\n","    # Train/F-score\n","    writer.add_scalar('train/f_score', f_score, i_start + i)\n","\n","  # --- VALIDATION ---\n","  with torch.no_grad():\n","    # Creazione liste per salvare i vari indici\n","    true_positive_counter=[]\n","    false_positive_counter=[]\n","    true_negative_counter=[]\n","    false_negative_counter=[]\n","    elements=[]\n","    loss=[]\n","\n","    # dati del validaiton\n","    for X, y in dataloader_validation:\n","      X[0]=X[0].cuda()\n","      X[1]=X[1].cuda()\n","      y = y.cuda()\n","\n","      elements.append(len(y))\n","\n","      # Ottengo i risultati dal modello\n","      output = model(X)\n","      loss_result = lossFunction(output,y)\n","      loss.append(loss_result.item())\n","\n","      # Definisco il valore di positività\n","      positive = output[:, 1] >= 0.5\n","\n","      # calcolo gli indici di prestazione ed aggiorno il rispettivo contatore\n","      # True positive\n","      true_positive=(positive==1)&(y==1)\n","      true_positive_counter.append(true_positive.count_nonzero().item()*1.0)\n","      # False positive\n","      false_positive=(positive==1)&(y==0)\n","      false_positive_counter.append(false_positive.count_nonzero().item()*1.0)\n","      # True negative\n","      true_negative=(positive==0)&(y==0)\n","      true_negative_counter.append(true_negative.count_nonzero().item()*1.0)\n","      # False negative\n","      false_negative=(positive==0)&(y==1)\n","      false_negative_counter.append(false_negative.count_nonzero().item()*1.0)\n","\n","    # Calcolo i vari indici\n","    #loss\n","    loss_final = np.average(loss, weights=elements)\n","    # True positive\n","    true_positive_result=np.sum(true_positive_counter)\n","    # True negative\n","    true_negative_result=np.sum(true_negative_counter)\n","    # False positive\n","    false_positive_result=np.sum(false_positive_counter)\n","    # False negative\n","    false_negative_result=np.sum(false_negative_counter)\n","    # Precision\n","    precision_final=true_positive_result / (true_positive_result + false_positive_result) if true_positive_result + false_positive_result > 0 else 0.0\n","    # Recall\n","    recall_final=true_positive_result / (true_positive_result + false_negative_result) if true_positive_result + false_negative_result > 0 else 0.0\n","    # F-score\n","    f_score_final=2 * precision_final * recall_final / (precision_final + recall_final) if precision_final + recall_final > 0 else 0.0\n","    # Accuracy\n","    accuracy_final=(true_positive_result+true_negative_result)/(true_positive_result+true_negative_result+false_positive_result+false_negative_result)\n","\n","    # Salvataggio dei risultati degli indici\n","    # Validation/Accuracy\n","    writer.add_scalar('validation/accuracy', accuracy_final, i_start + i)\n","    # Validation/Loss\n","    writer.add_scalar('validation/loss', loss_final, i_start + i)\n","    # Validation/Precision\n","    writer.add_scalar('validation/precision', precision_final, i_start + i)\n","    # Validation/Recall\n","    writer.add_scalar('validation/recall', recall_final, i_start + i)\n","    # Validation/F-score\n","    writer.add_scalar('validation/f_score', f_score_final, i_start + i)\n","\n","    return accuracy_final, loss_final, precision_final, recall_final, f_score_final"]},{"cell_type":"markdown","metadata":{"id":"Q_7anW3Blfrh"},"source":["# Preparare i parametri per i frame"]},{"cell_type":"markdown","source":["Preprocessing dei frame in accordo agli standard della SlowFast_R50 ed augmentations tramite le funzioni di Albumentations."],"metadata":{"id":"70GPXlAEyeQY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMLua_dXlfGf"},"outputs":[],"source":["# Preprocessing dei frame\n","preprocessing = albumentations.Sequential(\n","    [\n","        albumentations.SmallestMaxSize(max_size=256, always_apply=True),\n","        albumentations.CenterCrop(height=224, width=224, always_apply=True),\n","    ]\n",")\n","\n","# Augumentation dei frame\n","augumentation = albumentations.Compose([\n","    albumentations.OneOf([\n","        albumentations.ColorJitter(p=0.5),\n","        albumentations.GaussianBlur(p=0.5),\n","        albumentations.HorizontalFlip(p=1),\n","        albumentations.RandomBrightnessContrast(p=0.5),\n","        albumentations.HueSaturationValue(p=0.5)\n","    ], p=0.1)\n","])"]},{"cell_type":"markdown","source":["A causa della natura della SlowFast_R50, che gestisce sia uno slow path che un fast path, il numero di segmenti per ogni video è stato impostato a 32, come previsto dalla struttura della rete. Tuttavia, poiché la rete stessa gestisce il flusso temporale, abbiamo impostato il parametro \"num_segment\" uguale a 1. Ciò ci ha permesso di considerare frame consecutivi, mantenendo un approccio sequenziale nell'analisi dei video durante il training."],"metadata":{"id":"WHwYkv-zy0o9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zsis36EmQDW"},"outputs":[],"source":["#@title  { display-mode: \"form\" }\n","#@markdown Parametri per l'augumentation\n","#@markdown ---\n","\n","transform_probability = 1 #@param {type: \"number\"}\n","\n","#@markdown ---\n","#@title  { display-mode: \"form\" }\n","#@markdown Parametri per il Dataloader\n","#@markdown ---\n","\n","num_workers=2 #@param {type: \"number\"}\n","\n","batch_size=16 #@param {type: \"number\"}\n","\n","num_segments = 1 #@param {type:\"integer\"}\n","\n","frames_per_segment = 32 #@param {type:\"integer\"}\n","#@markdown ---"]},{"cell_type":"markdown","metadata":{"id":"6D4X3QMQbfCv"},"source":["# Preparare i parametri di training"]},{"cell_type":"markdown","source":["Di seguito sono riportati i parametri utilizzati durante il training del modello consigliato."],"metadata":{"id":"yfedA0vh0An1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"csxU4AogbfgP"},"outputs":[],"source":["#@title  { display-mode: \"form\" }\n","#@markdown Parametri per il training\n","\n","epochs = 125 #@param {type:\"integer\"}\n","k_folds = 5 #@param {type:\"integer\"}\n","\n","#@markdown ---\n","\n","learning_rate = 0.000015 #@param {type: \"number\"}\n","momentum = 0 #@param {type:\"number\"}\n","weight_decay = 0 #@param {type: \"number\"}\n","\n","#@markdown ---\n","\n","early_stopping = True #@param {type:\"boolean\"}\n","early_stopping_patience = 5 #@param {type:\"integer\"}\n","\n","#@markdown ---"]},{"cell_type":"markdown","metadata":{"id":"qMSWaWKWbtF8"},"source":["# Avviare TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFlM3Kg3SlpE"},"outputs":[],"source":["#!rm -r './executions'\n","!killall tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir ./executions/"]},{"cell_type":"markdown","metadata":{"id":"-FJT_xJCbyzc"},"source":["# Allenamento della rete"]},{"cell_type":"markdown","source":["In questa fase, stiamo eseguendo il training della rete.<br>\n","Abbiamo scelto di utilizzare l'ottimizzatore \"ADAM\" perché, a differenza di SGD (Stochastic Gradient Descent), è meno sensibile al learning rate. L'ottimizzatore ADAM adatta il learning rate per ogni parametro del modello, rendendo il processo di addestramento più stabile e riducendo la necessità di una scelta manuale accurata del learning rate. Questo ci permette di ottenere una maggiore efficienza nel training della rete e una migliore convergenza verso soluzioni ottimali."],"metadata":{"id":"GupccTs-0TeB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzX6B20IbybT"},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","# Altri parametri per il training\n","dataloader_params = {'pin_memory': True}\n","K_folds = [k_folds] # se si vogliono usare diversi tipi di divisione dei fold, inserli qui\n","\n","lossFunction = CrossEntropyLoss()\n","folder_path = \"executions\"\n","# Se la cartella esiste, la svuota\n","if os.path.exists(folder_path):\n","    shutil.rmtree(folder_path)  # Rimuovi la cartella e tutti i suoi contenuti\n","    os.makedirs(folder_path)  # Ricrea la cartella vuota\n","else:\n","    os.makedirs(folder_path)  # Crea la cartella se non esiste\n","\n","# Per ogni dimensione di fold\n","for K in K_folds:\n","  # Settiamo a 0 i vari incidi di performance del validaiton\n","  # Accuracy\n","  validation_accuracy_results = torch.zeros(K, epochs)\n","  # Loss\n","  validation_loss_results = torch.zeros(K, epochs)\n","  # Precision\n","  validation_precision_results = torch.zeros(K, epochs)\n","  # Recall\n","  validation_recall_results = torch.zeros(K, epochs)\n","  # F-score\n","  validation_f_score_results = torch.zeros(K, epochs)\n","\n","  # Nome dell'eserimento\n","  experiment_fold = 'exp-tot_folds-{}'.format(K)\n","\n","  # Path per la cartella dell'esperimento\n","  experiment_path = os.path.join(folder_path, experiment_fold)\n","  os.mkdir(experiment_path)\n","\n","  # Split dei fold\n","  k_fold_split(initial_path,kfold_path, K)\n","  # Per ogni fold\n","  for k in range(K):\n","    # Pulisco il modello\n","    clear_model()\n","    model.cuda()\n","\n","    # Definisco la loss minima\n","    min_loss = 1e10\n","\n","    # Definisco il nome dell'eserimento\n","    experiment_name = (experiment_fold + '-fold-{}').format(k+1)\n","    path = os.path.join(folder_path, experiment_fold, experiment_name)\n","    os.mkdir(path)\n","\n","    # Creazione dei dataloader di training e validation a partire dai fold\n","    [dataloader_train, dataloader_validation]=make_training_validation_dataloaders(kfold_path,K, k+1,\n","            preprocessing,augumentation,num_segments,frames_per_segment,transform_probability,\n","            batch_size,num_workers)# +1 perchè partono da 1\n","\n","    # Preparazione dell'optimizer e del writer per TensorFlow\n","    writer = SummaryWriter(path)\n","    optimizer = Adam(model.parameters(),\n","                    lr=learning_rate,\n","\n","                    weight_decay=weight_decay)\n","\n","    # Settaggio del contatore dell'early stopping\n","    early_stopping_counter = early_stopping_patience\n","\n","    # Esecuzione delle epoche\n","    for epoch in tqdm(range(epochs), desc=experiment_name):\n","      validation_accuracy, validation_loss, validation_precision, validation_recall,validation_f_score = one_epoch(model,dataloader_train,\n","                                dataloader_validation,lossFunction,optimizer,writer,epoch)\n","\n","      # Salvataggio degli indici di performance\n","      # Accuracy\n","      validation_accuracy_results[k, epoch] = validation_accuracy\n","      # Loss\n","      validation_loss_results[k, epoch] = validation_loss\n","      # Precision\n","      validation_precision_results[k, epoch] = validation_precision\n","      # Recall\n","      validation_recall_results[k, epoch] = validation_recall\n","      # F-score\n","      validation_f_score_results[k, epoch] = validation_f_score\n","\n","      # Salvataggio dei pesi relativi al modello con minor loss attuale\n","      if validation_loss < min_loss:\n","        min_loss = validation_loss\n","        print(\"Epoca: \"+str((epoch)))\n","        print(\"Min loss: \"+str(min_loss))\n","        torch.save(model.state_dict(), os.path.join(path, 'model.pth'))\n","        early_stopping_counter = early_stopping_patience\n","\n","      print(\"Accuracy: \"+str(validation_accuracy))\n","\n","      # Early Stopping\n","      if early_stopping and epoch > 0:\n","        if validation_loss >= validation_loss_results[k, epoch-1]:\n","          early_stopping_counter -= 1\n","        else:\n","          early_stopping_counter = early_stopping_patience\n","        if early_stopping_counter == 0:\n","          break\n","\n","  # Restore dei fold\n","  restore_k_fold(kfold_path,initial_path, K)\n","\n","  # Salvataggio dei risultati del validation\n","  # Accuracy\n","  torch.save(validation_accuracy_results, os.path.join(experiment_path, 'validation_accuracy_results.pth'))\n","  # Loss\n","  torch.save(validation_loss_results, os.path.join(experiment_path, 'validation_loss_results.pth'))\n","  # Precision\n","  torch.save(validation_precision_results, os.path.join(experiment_path, 'validation_precision_results.pth'))\n","  # Recall\n","  torch.save(validation_recall_results, os.path.join(experiment_path, 'validation_recall_results.pth'))\n","  # F-score\n","  torch.save(validation_f_score_results, os.path.join(experiment_path, 'validation_f_score_results.pth'))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}